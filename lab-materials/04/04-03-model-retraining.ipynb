{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cbdac0-3371-48af-aaa9-c40bbbcb62ff",
   "metadata": {},
   "source": [
    "# Retrain the YOLO model\n",
    "\n",
    "To retrain the YOLO model we need a prepared dataset of objects images with moderate and severe labels.  We have such a dataset (obtained from RoboFlow) that already contains annotated images, splitted into training and validation datasets.  We will use these training/validation sets to retrain our current YOLO model. A few info on the data structure needed for training:\n",
    "\n",
    "1. The encode classes of objects we want to teach our model to detect is 0-`moderate` and 1-`severe`.\n",
    "2. The dataset will be in its own folder, with 2 subfolders in it: `train` and `valid`.  Within each subfolder there will be 2 subfolders: `images` and `labels`.\n",
    "3. Each image has a corresponding annotation text file in the `labels` subfolder. The annotation text files have the same names as the image files.\n",
    "4. A dataset descriptor YAML file (data.yaml) points to the datasets and describes the object classes in them. This YAML file is passed to the `train` method of the model to start the training process.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602e0c59-42b1-4de4-9de2-d0875cf597c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you did not use the Workbench image designed for this Lab, you can uncomment and run the following line to install the required packages.\n",
    "# !pip install --no-cache-dir --no-dependencies -r requirements.txt\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc755b4b-db03-4b43-9da9-80ed8298417b",
   "metadata": {},
   "source": [
    "Next let's load a YOLO model 'yolo8m.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e595eb2-0e11-4dce-9d5c-2a2025ddcd6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = YOLO('yolov8m.pt')  # load a pretrained model (recommended for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d43616-2d25-419b-bd30-47a5aafb0d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get the training data\n",
    "\n",
    "We have provided the following 2 training data sets, available as zip files:  \n",
    "1) `accident-full.zip`   - to be used to fully re-train the model.\n",
    "2) `accident-sample.zip` - to be used to partially re-train the model when we don't have the time to fully re-train the model.\n",
    "\n",
    "During the workshop we will only use the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2657ceae-db2d-4d65-ae1c-e807254ee71b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5b2444ad2a46f79e2422c04acd4035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/5.44M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve a specific dataset\n",
    "def retrieve_dataset(dataset_type):\n",
    "\n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not os.path.exists(\"./datasets/\"):\n",
    "        os.makedirs(\"./datasets/\")\n",
    "\n",
    "    URL = f\"https://rhods-public.s3.amazonaws.com/sample-data/accident-data/accident-{dataset_type}.zip\"\n",
    "\n",
    "    # Check if the file exists, if not, download and unzip it\n",
    "    if not os.path.exists(f\"./datasets/accident-{dataset_type}.zip\"):\n",
    "        print(\"Downloading file...\")\n",
    "        response = requests.get(URL, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "        t = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "        with open(f'./datasets/accident-{dataset_type}.zip', 'wb') as f:\n",
    "            for data in response.iter_content(block_size):\n",
    "                t.update(len(data))\n",
    "                f.write(data)\n",
    "        t.close()\n",
    "    if os.path.exists(f\"./datasets/accident-{dataset_type}.zip\"):\n",
    "        print(\"Unzipping file...\")\n",
    "        with zipfile.ZipFile(f'./datasets/accident-{dataset_type}.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(path='./datasets/')\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "dataset_type = 'sample'\n",
    "# dataset_type = 'full' # Use this line instead if you want to retrieve the full dataset\n",
    "retrieve_dataset(dataset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7d5b4-4c29-4eff-b1ba-a9dddf382173",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Re-training our YOLO model\n",
    "\n",
    "Let's start by understanding what an 'epoch' is.  Machine learning models are trained with specific datasets passed through an algorithm. Each time the full dataset passes through the algorithm, it is said to have completed an **epoch**. Each **epoch** will further refine the training of the model.\n",
    "\n",
    "In the training code below we will set various parameters:  \n",
    "**results = model.train(data='./datasets/accident-sample/data.yaml', epochs=1, imgsz=640, batch=2)**\n",
    "\n",
    "- epochs: as we want to only demo the process, we will use only 1 epoch.\n",
    "- imgsz: this is the size of the images the model has to be fed with.\n",
    "- batch: this is the number of images that the algorithm will process simultaneously. The more the better results, but the more memory it consumes. As we are on constrained resources in this workshop, and this is only a training example, we keep it intentionally low at 2.\n",
    "\n",
    "In your training run, each **epoch<** will show a summary for both the training and validation phases: lines 1 and 2 show results of the training phase and lines 3 and 4 show the results of the validation phase for each epoch.  \n",
    "\n",
    "Execute the following cell to start re-training the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22954895-0c26-4ae0-93c1-259efd18e47e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.76 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.11.7 torch-2.2.2+cpu CPU (Intel Xeon Platinum 8259CL 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=./datasets/accident-sample/data.yaml, epochs=1, time=None, patience=100, batch=2, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/opt/app-root/src/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 75.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776854  ultralytics.nn.modules.head.Detect           [2, [192, 384, 576]]          \n",
      "Model summary: 295 layers, 25857478 parameters, 25857462 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /opt/app-root/src/sentiments/lab-materials/04/datasets/accident-sample/train/labels... 22 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 1190.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /opt/app-root/src/sentiments/lab-materials/04/datasets/accident-sample/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app-root/src/sentiments/lab-materials/04/datasets/accident-sample/valid/labels... 22 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 1543.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /opt/app-root/src/sentiments/lab-materials/04/datasets/accident-sample/valid/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      1.156      6.874      1.129          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [01:08<00:00,  6.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:27<00:00,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22         25      0.574      0.214     0.0766     0.0337\n",
      "\n",
      "1 epochs completed in 0.028 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.11.7 torch-2.2.2+cpu CPU (Intel Xeon Platinum 8259CL 2.50GHz)\n",
      "Model summary (fused): 218 layers, 25840918 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22         25      0.575      0.214     0.0776     0.0318\n",
      "              moderate         22         21       0.15      0.429      0.155     0.0636\n",
      "                severe         22          4          1          0          0          0\n",
      "Speed: 3.5ms preprocess, 1186.5ms inference, 0.0ms loss, 200.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "results = model.train(data='./datasets/accident-sample/data.yaml', epochs=1, imgsz=640, batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45050c91-fb6c-4f14-a4d1-e72d3a40538f",
   "metadata": {},
   "source": [
    "**Note**: after a standard re-training, if we are happy with the results, we could export our model to the ONNX format. \n",
    "(you would replace trainX by the traininig session you want to use in the following command)\n",
    "\n",
    "ObjDetOXModel = YOLO(\"best.pt\").export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d622fb13-d32b-4bd1-899e-1e8eb7798a88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.11.7 torch-2.2.2+cpu CPU (Intel Xeon Platinum 8259CL 2.50GHz)\n",
      "YOLOv8-config summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.3s, saved as 'best.onnx' (11.7 MB)\n",
      "\n",
      "Export complete (3.5s)\n",
      "Results saved to \u001b[1m/opt/app-root/src/sentiments/lab-materials/04\u001b[0m\n",
      "Predict:         yolo predict task=detect model=best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=best.onnx imgsz=640 data=/opt/app-root/src/baggage-images/config.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'best.onnx'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Export model in onnx format to be deployed in model server\n",
    "# Load the trained YOLO model\n",
    "model = YOLO(\"best.pt\")\n",
    "\n",
    "# Export the model to ONNX format\n",
    "model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f7412-0b9b-46d1-b2f6-ded479b20cb7",
   "metadata": {},
   "source": [
    "## Interpreting our Training Results\n",
    "\n",
    "A full description of the process and the results for a training against the **full dataset** is available in the Lab instructions.\n",
    "\n",
    "Now that we have retrained our model let's test it against images with objects suited more broadly for the aviation industry!\n",
    "\n",
    "**Please open the notebook `04-04-accident-recog.ipynb`**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
